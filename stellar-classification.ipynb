{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stellar classification\n",
    "\n",
    "Data source: [Kaggle](https://www.kaggle.com/vinesmsuic/preprocessing-the-stardataset)\n",
    "\n",
    "Stellar Classification uses the spectral data of stars to categorize them into different categories. The modern stellar classification system is known as the Morgan–Keenan (MK) classification system. It uses the old HR classification system to categorize stars with their chromaticity and uses Roman numerals to categorize the star’s size.\n",
    "\n",
    "In this Dataset, we will be using Absolute Magnitude and B-V Color Index to Identify Giants and Dwarfs.\n",
    "\n",
    "Columns:\n",
    "- Vmag - Visual Apparent Magnitude of the Star\n",
    "- Plx - Distance Between the Star and the Earth\n",
    "- e_Plx - Standard Error of `Plx` (Drop the Row if you find the e_Plx is too high!)\n",
    "- B-V - B-V color index.\n",
    "- SpType - Spectral type\n",
    "- Amag - Absolute Magnitude of the Star\n",
    "- TargetClass - Whether the Star is Dwarf (0) or Giant (1)\n",
    "\n",
    "Goal:\n",
    "Predict star type\n",
    "\n",
    "For the predictions, we will use `B-V` and `Amag` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # seaborne is a package built on top of matplotlib.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set() # activate seaborn to override all the matplotlib graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"Star39552_balanced.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Target\n",
    "y = df['TargetClass']\n",
    "\n",
    "# Select Features\n",
    "x = df[['B-V','Amag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vmag</th>\n",
       "      <th>Plx</th>\n",
       "      <th>e_Plx</th>\n",
       "      <th>B-V</th>\n",
       "      <th>Amag</th>\n",
       "      <th>TargetClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39552.000000</td>\n",
       "      <td>39552.000000</td>\n",
       "      <td>39552.000000</td>\n",
       "      <td>39552.000000</td>\n",
       "      <td>39552.000000</td>\n",
       "      <td>39552.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.921309</td>\n",
       "      <td>7.117378</td>\n",
       "      <td>1.109705</td>\n",
       "      <td>0.744336</td>\n",
       "      <td>16.050687</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.308857</td>\n",
       "      <td>12.446291</td>\n",
       "      <td>0.788133</td>\n",
       "      <td>0.513987</td>\n",
       "      <td>2.443937</td>\n",
       "      <td>0.500006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.620000</td>\n",
       "      <td>-27.840000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.210000</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.358000</td>\n",
       "      <td>14.756514</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.160000</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>16.020827</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.830000</td>\n",
       "      <td>8.232500</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>1.129000</td>\n",
       "      <td>17.590542</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.850000</td>\n",
       "      <td>772.330000</td>\n",
       "      <td>40.630000</td>\n",
       "      <td>3.440000</td>\n",
       "      <td>30.449015</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Vmag           Plx         e_Plx           B-V          Amag  \\\n",
       "count  39552.000000  39552.000000  39552.000000  39552.000000  39552.000000   \n",
       "mean       7.921309      7.117378      1.109705      0.744336     16.050687   \n",
       "std        1.308857     12.446291      0.788133      0.513987      2.443937   \n",
       "min       -0.620000    -27.840000      0.420000     -0.400000     -0.350000   \n",
       "25%        7.210000      2.430000      0.800000      0.358000     14.756514   \n",
       "50%        8.160000      4.440000      0.990000      0.703000     16.020827   \n",
       "75%        8.830000      8.232500      1.230000      1.129000     17.590542   \n",
       "max       12.850000    772.330000     40.630000      3.440000     30.449015   \n",
       "\n",
       "        TargetClass  \n",
       "count  39552.000000  \n",
       "mean       0.500000  \n",
       "std        0.500006  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.500000  \n",
       "75%        1.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into two parts, 25% will be used for testing and the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use scaler to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(classifier, x_train, y_train, x_test, y_test):\n",
    "    tic = time.perf_counter()\n",
    "    classifier.fit(x_train, y_train)\n",
    "    toc = time.perf_counter()\n",
    "\n",
    "    print(f\"Fitting took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    toc = time.perf_counter()\n",
    "\n",
    "    print(f\"Predicting on the test data took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "    print()\n",
    "    print('The score on train dataset is {}'.format(classifier.score(x_train, y_train)))\n",
    "    print('The test accuracy is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "    print()\n",
    "    \n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix : \\n\", cm)    \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A random Tree\n",
    "\n",
    "Decision Trees are a non-parametric supervised learning classifiers. The model predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 0.0483 seconds\n",
      "Predicting on the test data took 0.0014 seconds\n",
      "\n",
      "The score on train dataset is 0.8816073354908306\n",
      "The test accuracy is 0.8782362459546925\n",
      "\n",
      "Confusion Matrix : \n",
      " [[4159  780]\n",
      " [ 424 4525]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree = tree.DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "tree.fit(x_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Fitting took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y_pred = tree.predict(x_test)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Predicting on the test data took {toc - tic:0.4f} seconds\")\n",
    "print()\n",
    "\n",
    "print('The score on train dataset is {}'.format(tree.score(x_train, y_train)))\n",
    "print('The test accuracy is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix : \\n\", cm)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree was really fast and it classifies our data fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification\n",
    "\n",
    "A random forest is an ensamble learning method that uses a number of decision tree classifiers on various sub-samples of the dataset. These decision trees than vote to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 4.7286 seconds\n",
      "Predicting on the test data took 0.2432 seconds\n",
      "\n",
      "The score on train dataset is 0.9997977346278317\n",
      "The test accuracy is 0.8631674757281553\n",
      "\n",
      "Confusion Matrix : \n",
      " [[4233  706]\n",
      " [ 647 4302]]\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "\n",
    "tic = time.perf_counter()\n",
    "forest.fit(x_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Fitting took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y_pred = forest.predict(x_test)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Predicting on the test data took {toc - tic:0.4f} seconds\")\n",
    "print()\n",
    "\n",
    "print('The score on train dataset is {}'.format(forest.score(x_train, y_train)))\n",
    "print('The test accuracy is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix : \\n\", cm)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy without any parameters is exceptionaly high, but it drops dramatically, during testing. This suggests that our model is overfitting. We will tweak the classifier's parameters to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 1.7115 seconds\n",
      "Predicting on the test data took 0.0544 seconds\n",
      "\n",
      "The score on train dataset is 0.8832928802588996\n",
      "The test accuracy is 0.8789441747572816\n",
      "\n",
      "Confusion Matrix : \n",
      " [[4195  744]\n",
      " [ 453 4496]]\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(max_depth=5)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "forest.fit(x_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Fitting took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y_pred = forest.predict(x_test)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Predicting on the test data took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "print()\n",
    "print('The score on train dataset is {}'.format(forest.score(x_train, y_train)))\n",
    "print('The test accuracy is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the maximal depth of trees lowered the training accuracy, but the model did a little bit better on the test data. This also decreased the training and predicting times. Unfortunatelly, it doesn't provide any advatage over using a single decision tree. The accuracy is comparable and training times are much longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. \n",
    "\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of probability. Gaussian Naive Bayes assumes Gaussian distribution. In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 0.0065 seconds\n",
      "Predicting on the test data took 0.0051 seconds\n",
      "\n",
      "The score on train dataset is 0.8806634304207119\n",
      "The test accuracy is 0.8777305825242718\n",
      "Confusion Matrix : \n",
      " [[4151  788]\n",
      " [ 421 4528]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "tic = time.perf_counter()\n",
    "gnb.fit(x_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Fitting took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y_pred = gnb.fit(x_train, y_train).predict(x_test)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Predicting on the test data took {toc - tic:0.4f} seconds\")\n",
    "print()\n",
    "\n",
    "\n",
    "print('The score on train dataset is {}'.format(gnb.score(x_train, y_train)))\n",
    "print('The test accuracy is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of Naive Bayes is comparable to that of the decision tree and the random forest, and training times comparable to the decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression, despite its name, is a linear model for classification rather than regression. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 0.0295 seconds\n",
      "Predicting on the test data took 0.0153 seconds\n",
      "\n",
      "The score on train dataset is 0.8797532362459547\n",
      "The test accuracy is 0.8767192556634305\n",
      "Confusion Matrix : \n",
      " [[4229  710]\n",
      " [ 509 4440]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "tic = time.perf_counter()\n",
    "log_reg.fit(x_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Fitting took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y_pred = log_reg.fit(x_train, y_train).predict(x_test)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Predicting on the test data took {toc - tic:0.4f} seconds\")\n",
    "print()\n",
    "\n",
    "\n",
    "print('The score on train dataset is {}'.format(log_reg.score(x_train, y_train)))\n",
    "print('The test accuracy is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression was so far the fastest of all the methods, which we employed and the accuracy was comparable with the previous algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "\n",
    "This is an ensamble learning technique that combines different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 1.5379 seconds\n",
      "Predicting on the test data took 1.4454 seconds\n",
      "\n",
      "The score on train dataset is 0.8820118662351673\n",
      "The test accuracy is 0.879247572815534\n",
      "Confusion Matrix : \n",
      " [[4184  755]\n",
      " [ 439 4510]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_reg), ('rf', forest), ('gnb', gnb)], voting='hard')\n",
    "\n",
    "tic = time.perf_counter()\n",
    "voting_clf = voting_clf.fit(x_train, y_train)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Fitting took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y_pred = voting_clf.fit(x_train, y_train).predict(x_test)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Predicting on the test data took {toc - tic:0.4f} seconds\")\n",
    "print()\n",
    "\n",
    "\n",
    "print('The score on train dataset is {}'.format(voting_clf.score(x_train, y_train)))\n",
    "print('The test accuracy is {}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We used several classifiers (namely: `DecisionTreeClassifier`, `RandomForestClassifier`, `GaussianNB`, and `LogisticRegression`)to fit the Stellar classification data. At the end we combined our classifiers into a single one using the `VotingClassifier`. \n",
    "\n",
    "The accuracy of all the classifier we've used was similar and therefore, the decision which one we should use hast to depend on something else. We found that the fitting and the prediction times varied a lot. The fastest method was `GaussianNB`, followed by `LogisticRegression`. The third place goes to `DecisionTreeClassifier`.\n",
    "\n",
    "The `RandomForestClassifier` and `VotingClassifier` were the slowest and they provided no additional benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
